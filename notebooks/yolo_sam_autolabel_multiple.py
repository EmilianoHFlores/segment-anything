# -*- coding: utf-8 -*-
"""YOLO_SAM_autolabel_multiple.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v1iu8aQ-F8uVBcj8-fXuRE6_s1urObMa
"""

from google.colab import drive
drive.mount('/content/drive')

!git clone https://github.com/EmilianoHFlores/segment-anything

# Commented out IPython magic to ensure Python compatibility.
# %cd segment-anything/notebooks
import os
!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import torch
import cv2
import os
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# Model
model = torch.hub.load("ultralytics/yolov5", "yolov5x")  # or yolov5n - yolov5x6, custom



# Commented out IPython magic to ensure Python compatibility.
# Images
#img = "data/images/zidane.jpg" # or file, Path, PIL, OpenCV, numpy, list
imgPath0 = "images/prueba2.jpg"

img = cv2.imread(imgPath0)
# %cd segment-anything/notebooks

results = model(img)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# Results
showimg = img.copy()
for *xyxy, conf, cls in results.pandas().xyxy[0].itertuples(index=False):
    print(f"Predicted {cls} at {[round(elem, 2) for elem in xyxy ]} with confidence {conf:.2f}.")
    showimg = cv2.rectangle(showimg, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0, 255, 0), 2)
    showimg = cv2.putText(showimg, f"{cls} {conf:.2f}", (int(xyxy[0]), int(xyxy[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Visualize on matplotlib
plt.imshow(cv2.cvtColor(showimg, cv2.COLOR_BGR2RGB))
plt.axis('off') 
plt.show()

def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamPredictor
sam_model = "h"

if sam_model =="h":
  sam_checkpoint = "/content/segment-anything/notebooks/sam_vit_h_4b8939.pth"
  model_type = "vit_h"
else:
  sam_checkpoint = "/content/segment-anything/notebooks/sam_vit_l_0b3195.pth"
  model_type = "vit_l"

device = "cuda"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

predictor = SamPredictor(sam)

scale_percent = 100 # percent of original size
width = int(img.shape[1] * scale_percent / 100)
height = int(img.shape[0] * scale_percent / 100)
dim = (width, height)
  
# resize image
resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)

predictor.set_image(resized)

image_bboxes = []
image_xywh = []

for *xyxy, conf, cls in results.pandas().xyxy[0].itertuples(index=False):
  if cls in ['cup', 'bottle', 'vase'] or True:
    print(f"Predicted {cls} at {[round(elem, 2) for elem in xyxy ]} with confidence {conf:.2f}.")
    image_bboxes.append(np.array([xyxy[0], xyxy[1], xyxy[2], xyxy[3]]))
    image_xywh.append([xyxy[0], xyxy[1], xyxy[2]-xyxy[0], xyxy[3]-xyxy[1]])

masks = []

for image_bbox in image_bboxes:
  mask, _, _ = predictor.predict(
      point_coords=None,
      point_labels=None,
      box=image_bbox,
      multimask_output=False,
  )
  masks.append(mask)

plt.figure(figsize=(10, 10))
showimg = resized.copy()

for i, mask in enumerate(masks):
  print(f"drawing mask{i}")
  print(type(mask[0]))
  contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
  cv2.drawContours(showimg, contours, -1, (255,0,255), 3)



plt.imshow(cv2.cvtColor(showimg, cv2.COLOR_BGR2RGB))

print(masks[0].shape[-2:])
plt.axis('off')
plt.show()

# Creating annotation in COCO format
#{"id": 0, "file_name": "0.jpg", "height": 480, "width": 736}
images=[]
annotations=[]
categories=[]

category_id = 0
category_name = "laptop"

image_0 ={"id": 0, "file_name": "0.jpg", "height": 480, "width": 736}

img_id=0
anno_id=0

imgPaths = os.listdir('images')
print(imgPaths)

for imgPath in imgPaths:
  print(f"Processing image: {imgPath}")
  img = cv2.imread(f"images/{imgPath}")
  if img is None:
    continue
  results = model(img)

  image_bboxes = []
  xywh = []

  #Get yolo results
  for *xyxy, conf, cls in results.pandas().xyxy[0].itertuples(index=False):
    #run for each detection
    if cls in ['cup', 'bottle', 'vase']:
      print(f"Predicted {cls} at {[round(elem, 2) for elem in xyxy ]} with on img {imgPath}.")
      image_bbox = (np.array([xyxy[0], xyxy[1], xyxy[2], xyxy[3]]))
      xywh = [xyxy[0], xyxy[1], xyxy[2]-xyxy[0], xyxy[3]-xyxy[1]]

      predictor.set_image(img)
  
      mask, _, _ = predictor.predict(
          point_coords=None,
          point_labels=None,
          box=image_bbox,
          multimask_output=False,
      )

      contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
      cv2.drawContours(img, contours, -1, (255,0,255), 3)

      segmentation = []
      aux_segmentation = []
      for contour in contours:
        polygon = contour.flatten().tolist()
        if len(polygon) % 2 != 0:
            print("Error: polygon has an odd number of coordinates")
            continue
        polygon = np.array(polygon).reshape((-1, 2))
        aux_segmentation.append(polygon)
      polygon_area = int(cv2.contourArea(aux_segmentation[0]))

      for sublist in aux_segmentation[0]:
        for item in sublist:
          segmentation.append(int(item))
      
      annotations.append({"id": anno_id,"image_id": img_id,"category_id": category_id,"bbox": [int(xywh[0]), int(xywh[1]), int(xywh[2]), int(xywh[3])],"segmentation": segmentation,"area": polygon_area,"iscrowd": 0})
      anno_id = anno_id+1
  print(f"Imgid: {img_id}")
  images.append({"id": img_id, "file_name": imgPath,"height": img.shape[0],"width": img.shape[1]})
  img_id = img_id+1
  print(f"Finished image {imgPath}")

  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
  plt.show()

categories = []
categories.append({"id": 0,"name": "red bottles"})
print(annotations)
  



#--------------------------------------------------
'''
    for img in fg_imgs:
        fg_img=img[1]

        # Load the image with alpha channel
        image = cv2.imread(img[2], cv2.IMREAD_UNCHANGED)

        # Extract the alpha channel and convert it to a binary mask
        alpha = image[:,:,3]
        mask = alpha > 0

        # Find contours and polygonal approximations
        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # Format the segmentation coordinates in COCO format
        segmentation = []
        aux_segmentation = []
        for contour in contours:
            polygon = contour.flatten().tolist()
            if len(polygon) % 2 != 0:
                print("Error: polygon has an odd number of coordinates")
                continue
            polygon = np.array(polygon).reshape((-1, 2))
            aux_segmentation.append(polygon)
        polygon_area = cv2.contourArea(aux_segmentation[0])
        for sublist in aux_segmentation[0]:
            for item in sublist:
                segmentation.append(item)


        # Calculate the maximum allowed position for the top-left corner
        max_x = bg_img.width - fg_img.width
        max_y = bg_img.height - fg_img.height
        max_area = fg_img.width * fg_img.height

            # Generate a random location until an unoccupied area is found that meets the overlap limit

        for i in range (10):
            x = random.randint(0, max_x)
            y = random.randint(0, max_y)

            # Calculate the overlap area
            overlap_area = np.sum(occupied[y:y+fg_img.height, x:x+fg_img.width])

            # Check if the area is unoccupied and the overlap limit is not exceeded
            if (max_overlap_area - max_area) >= np.sum(occupied):
                break
            if i==10:
                continue

        for i in range(0, len(segmentation)):
            if i % 2:
                i=i+x
            else :
                i=i+y
                
        # Update the occupied array
        occupied[y:y+fg_img.height, x:x+fg_img.width] = 1
        bg_img.paste(fg_img, (x, y), fg_img)
        annotations.append({"id": anno_id,"image_id": img_id,"category_id": annotations_dict[img[0]],"bbox": [x, y, fg_img.width, fg_img.height],"segmentation": segmentation,"area": polygon_area,"iscrowd": 0})
        anno_id=anno_id+1
        #draw = ImageDraw.Draw(bg_img)
        #draw.rectangle((x, y, x+fg_img.width, y+fg_img.height), outline='red', width=3)
    bg_img.save(str(img_id)+".jpg", quality=100)
    images.append({"id": img_id, "file_name": str(img_id)+".jpg","height": bg_img.height,"width": bg_img.width})
    img_id=img_id+1

    annotations.append({"id": anno_id,"image_id": img_id,"category_id": annotations_dict[img[0]],"bbox": [x, y, fg_img.width, fg_img.height],"segmentation": segmentation,"area": area,"iscrowd": 0})

    '''

# Define the COCO dictionary
coco_dict = {
    "images": images,
    "annotations": annotations,
    "categories": categories
}

print(coco_dict)

import json
json_annotation = json.dumps(coco_dict)
with open('annotations.json', 'w') as f:
  f.write(json_annotation)

!cat annotations.json

# Download the file.
from google.colab import files
files.download('annotations.json')